x-litellm-common: &litellm-common
  image: ghcr.io/berriai/litellm:main-latest
  restart: unless-stopped
  expose:
    - "4000"
  volumes:
    - ../litellm/litellm_config.yaml:/app/config.yaml:ro
    - prometheus-multiproc:/prometheus_multiproc
  environment:
    - PROMETHEUS_MULTIPROC_DIR=/prometheus_multiproc
    - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
    - REDIS_HOST=${REDIS_HOST}
    - REDIS_PORT=${REDIS_PORT}
    - REDIS_CACHE_TTL=${REDIS_CACHE_TTL}
    - REDIS_CACHE_NAMESPACE=${REDIS_CACHE_NAMESPACE}
    - MODEL_NAME=${MODEL_NAME}
    - MODEL_PATH=${MODEL_PATH}
    - VLLM_API_BASE_1=${VLLM_API_BASE_1}
    - VLLM_API_BASE_2=${VLLM_API_BASE_2}
    - VLLM_API_KEY=${VLLM_API_KEY}
  networks:
    - internal
    - monitoring
  command: ["--config", "/app/config.yaml", "--port", "4000", "--detailed_debug"]
  healthcheck:
    test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness')\" "]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s

services:

  # LiteLLM proxies to vLLM (3 dedicated containers for load spreading)
  litellm-1:
    <<: *litellm-common
    container_name: litellm-proxy-1

volumes:
  prometheus-multiproc:
    driver: local